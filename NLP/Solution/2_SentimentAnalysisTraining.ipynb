{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sentiment Analysis - Training Routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Firstly, set up the path to the (preprocessed) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path to the preprocessed data\n",
    "import os\n",
    "\n",
    "fileDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "absFilePathToPreprocessedDataset = os.path.join(fileDir, '../Data/training.1600000.processed.noemoticon_preprocessed.csv')\n",
    "pathToPreprocessedDataset = os.path.abspath(os.path.realpath(absFilePathToPreprocessedDataset))\n",
    "print (pathToPreprocessedDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the device to run the training on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step #1:** Instantiate the dataset\n",
    "\n",
    "Instantiate the dataset from the provided dataset path. The dataset is responsible for instantiating the used vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Common.TwitterDataset import TwitterDataset\n",
    "\n",
    "# instantiate the dataset\n",
    "dataset = TwitterDataset.load_dataset_and_make_vectorizer(pathToPreprocessedDataset)\n",
    "\n",
    "# get the vectorizer\n",
    "vectorizer = dataset.get_vectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step #2: Instantiate the model\n",
    "\n",
    "Instantiate the model and move it to tehe desired device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.ModelPerceptron import SentimentClassifierPerceptron\n",
    "\n",
    "# instantiate the model\n",
    "model = SentimentClassifierPerceptron(num_features=len(vectorizer.text_vocabulary), output_dim=2)\n",
    "\n",
    "# send model to appropriate device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step #3: Instantiate the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step #4: Instantiate the optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learningRate = 0.001\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus #1: Define how to calculate accuracy of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_accuracy(output, labels):\n",
    "    probability_values, indices = F.softmax(output, dim=1).max(dim=1)\n",
    "\n",
    "    correct = (indices == labels).float().sum()\n",
    "\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "The training loop uses the objects that are instantiated in the previous step to update model parameters so that its performance improves over time.\n",
    "\n",
    "The training loop is composed of two loops: an inner loop over minibatches in the dataset, and an outer loop which repeat the inner loop a predefined number of times (<code>num_epochs</code>). In the innter loop, losses are calculated for each minibatch and the optimizer is used to update the model parameters.\n",
    "\n",
    "In each epoch, the model is firstly trained on the training set: the training dataset is devided into batches and following 5 steps are repeated for each batch: \n",
    "- **Step #1**: Zero the gradients (clear the information about gradients from previous step)\n",
    "- **Step #2**: Calculate the model output\n",
    "- **Step #3**: Compute the loss, when compared with labels\n",
    "- **Step #4**: Use the loss to calculate and backpropagate gradients\n",
    "- **Step #5**: Use optimize to update weights of the model\n",
    "\n",
    "After the inner loop over training batches, the similar loop is done over validation data. The main difference is that validation data is not used to update model weights, it is just used to calculate its performance. Therefore, it has 3 steps, repeated for each batch:\n",
    "- **Step #1**: Calculate the model output\n",
    "- **Step #2**: Compute the loss, when compared with labels\n",
    "- **Step #3**: Compute the accuracy, when comapred with the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Common.Trainer import Trainer\n",
    "\n",
    "sentiment_analysis_trainer = Trainer(\n",
    "    dataset=dataset,\n",
    "    model=model,\n",
    "    loss_func=loss_func,\n",
    "    optimizer=optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the chosen number of epochs\n",
    "num_epochs = 50\n",
    "# setup the chosen batch size\n",
    "batch_size = 64\n",
    "\n",
    "report = sentiment_analysis_trainer.train(num_epochs=num_epochs, batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the training results\n",
    "\n",
    "#### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(report[\"train_loss\"])\n",
    "plt.title(\"Training Set Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(report[\"train_accuracy\"])\n",
    "plt.title(\"Training Set Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(report[\"validation_loss\"])\n",
    "plt.title(\"Validation Set Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(report[\"validation_accuracy\"])\n",
    "plt.title(\"Validation Set Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.13 ('NLPWorkshop')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "orig_nbformat": 2,
  "pygments_lexer": "ipython3",
  "version": 3,
  "vscode": {
   "interpreter": {
    "hash": "1683e64cef153fb31db34d99283364a93733aa00d7ace08c836c28b3309d15bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
