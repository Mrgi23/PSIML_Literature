{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Sentiment Analysis - BERT Classification\n",
    "\n",
    "**BERT** is a bidirectional model that is based on transformer architecture. The model is pre-trained on two unsupervised tasks, masked language modeling and next sentence prediction. It is commonly used as an encoder model for different downstream tasks, to provide vector representation of the input text.<br/>\n",
    "The decoder part, that is responsible for producing a prediction for the task, should be added separately, depending on the task. For the text classification task, decoder part usually contains few linear layers. <br/><br/>\n",
    "\n",
    "The goal of this exercise is to showcase how to use pre-trained BERT model for text classification. We use the standard implementation from [Hugging Face transformer library](https://huggingface.co/transformers/model_doc/bert.html).<br/>\n",
    "We explain how to prepare data, load the model, execute the model and evaluate the results. This notebook does not cover fine-tuning BERT model for specific downstream task, but it is highly recommended to do this exercise as a homework task, to fully understand the capabilities of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Firstly, set up the path to the (preprocessed) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the preprocessed data\n",
    "import os\n",
    "\n",
    "fileDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "absFilePathToPreprocessedDataset = os.path.join(fileDir, '../Data/training.1600000.processed.noemoticon_preprocessed.csv')\n",
    "pathToPreprocessedDataset = os.path.abspath(os.path.realpath(absFilePathToPreprocessedDataset))\n",
    "print(pathToPreprocessedDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up which device to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Common.TwitterDataset import TwitterDataset\n",
    "\n",
    "# Step #1: Instantiate the dataset\n",
    "dataset = TwitterDataset.load_dataset_and_make_vectorizer(pathToPreprocessedDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize BertTokenizer, that is based on WordPiece tokenization. It encodes the input text in the expected format and encapsulates vocabulary of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the batch size that should be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the chosen batch size\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data loaders for all three datasets.<br/>\n",
    "The following 2 steps are repeated for each dataset:<br/>\n",
    "* Iterate through the dataset to encode each tweet individually (tokenization + vectorization)\n",
    "* Group tweets in batches with <code>batch_size</code> elements, to create a DataLoader object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BERTModel.BERTDataLoader import prepare_dataloader\n",
    "\n",
    "train_dataloader = prepare_dataloader(tokenizer, dataset.train_df, batch_size)\n",
    "validation_dataloader = prepare_dataloader(tokenizer, dataset.validation_df, batch_size)\n",
    "test_dataloader = prepare_dataloader(tokenizer, dataset.test_df, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert for Sequence Classification\n",
    "\n",
    "We load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Step #2: Instantiate the model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    # use weights from pretrained 12-layer BERT model, with an uncased vocab.\n",
    "    pretrained_model_name_or_path=\"bert-base-uncased\",\n",
    "    num_labels=2,  # the number of output labels\n",
    "    output_attentions=False,  # whether the model returns attentions weights.\n",
    "    output_hidden_states=False,  # whether the model returns all hidden-states.\n",
    ")\n",
    "# send model to appropriate device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the results\n",
    "\n",
    "We run the model inference on the specific dataloader to evaluate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BERTModel.BERTPredictor import predict\n",
    "\n",
    "y_predicted = dataset.test_df.text.apply(lambda x: predict(text=x, model=model, tokenizer=tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More detailed evaluation on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RunHelper import print_evaluation_report\n",
    "\n",
    "print_evaluation_report(y_labeled=dataset.test_df.target, y_predicted=y_predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('NLPWorkshop')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "eff8d7d51e26047d069efc853c99b9bd7989bb88649bcc220bbb87d203046f2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
