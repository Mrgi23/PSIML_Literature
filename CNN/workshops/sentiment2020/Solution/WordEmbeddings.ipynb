{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the path to GloVe pretrained embeddings file. This file can be downloaded from here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "c:\\Users\\v-tastan\\source\\repos\\PetnicaNLPWorkshop\\Data\\glove.6B.100d.txt\n"
    }
   ],
   "source": [
    "# path to the downloaded embeddings file\n",
    "import os\n",
    "\n",
    "fileDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "absFilePathToGloVe = os.path.join(fileDir, '../Data/glove.6B.100d.txt')\n",
    "pathToGloveEmbeddings = os.path.abspath(os.path.realpath(absFilePathToGloVe))\n",
    "print (pathToGloveEmbeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the <code>PreTrainedEmbeddigs</code> class, that is used to efficiently load and process embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PreTrainedEmbeddings import PreTrainedEmbeddings\n",
    "\n",
    "embeddings = PreTrainedEmbeddings.from_embeddings_file(pathToGloveEmbeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the loaded pretrained embedding vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0.26688  ,  0.39632  ,  0.6169   , -0.77451  , -0.1039   ,\n        0.26697  ,  0.2788   ,  0.30992  ,  0.0054685, -0.085256 ,\n        0.73602  , -0.098432 ,  0.5479   , -0.030305 ,  0.33479  ,\n        0.14094  , -0.0070003,  0.32569  ,  0.22902  ,  0.46557  ,\n       -0.19531  ,  0.37491  , -0.7139   , -0.51775  ,  0.77039  ,\n        1.0881   , -0.66011  , -0.16234  ,  0.9119   ,  0.21046  ,\n        0.047494 ,  1.0019   ,  1.1133   ,  0.70094  , -0.08696  ,\n        0.47571  ,  0.1636   , -0.44469  ,  0.4469   , -0.93817  ,\n        0.013101 ,  0.085964 , -0.67456  ,  0.49662  , -0.037827 ,\n       -0.11038  , -0.28612  ,  0.074606 , -0.31527  , -0.093774 ,\n       -0.57069  ,  0.66865  ,  0.45307  , -0.34154  , -0.7166   ,\n       -0.75273  ,  0.075212 ,  0.57903  , -0.1191   , -0.11379  ,\n       -0.10026  ,  0.71341  , -1.1574   , -0.74026  ,  0.40452  ,\n        0.18023  ,  0.21449  ,  0.37638  ,  0.11239  , -0.53639  ,\n       -0.025092 ,  0.31886  , -0.25013  , -0.63283  , -0.011843 ,\n        1.377    ,  0.86013  ,  0.20476  , -0.36815  , -0.68874  ,\n        0.53512  , -0.46556  ,  0.27389  ,  0.4118   , -0.854    ,\n       -0.046288 ,  0.11304  , -0.27326  ,  0.15636  , -0.20334  ,\n        0.53586  ,  0.59784  ,  0.60469  ,  0.13735  ,  0.42232  ,\n       -0.61279  , -0.38486  ,  0.35842  , -0.48464  ,  0.30728  ])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.get_embedding(word=\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the core features of word embeddings is that they should encode syntactic and semantic relationships that manifest as regularities in word use. One of the most common way to explore the semantic repationships encoded in word embeddings is a method called \"analogy task\". There are three words provided and you should determine the fourth word, that has the same relationship to the third word, as the first two words have.\n",
    "\n",
    "If we observe words purely as vectors in some vector spaces, the difference between vectors <code>word2</code> and <code>word1</code> encodes the relationship between these two words. That means that the same difference should be between vectors <code>word4</code> and <code>word3</code>, as they should have the analoguos relationship. Therefore, the vector correspoding to the fourth word is calculated as <code>word3 + (word2 - word1)</code>. Doing a neaest neighbor query among vectors correspoding to the existing words, for this result vector, solves the analogy task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_print_analogy(embeddings, word1, word2, word3, number_analogies=5):\n",
    "\n",
    "    vector1 = embeddings.get_embedding(word1)\n",
    "    vector2 = embeddings.get_embedding(word2)\n",
    "    vector3 = embeddings.get_embedding(word3)\n",
    "\n",
    "    spatial_relationship = vector2 - vector1\n",
    "\n",
    "    vector4 = vector3 + spatial_relationship\n",
    "\n",
    "    closest_words = embeddings.get_words_closest_to_vector(vector=vector4, n=number_analogies)\n",
    "\n",
    "    existing_words = set([word1, word2, word3])\n",
    "    closest_words = [word for word in closest_words if word not in existing_words]\n",
    "\n",
    "    if len(closest_words) == 0:\n",
    "        print(\"Could not find the nearest neighbors for the vector!\")\n",
    "        return\n",
    "\n",
    "    for word4 in closest_words:\n",
    "        print(\"{} : {} :: {} : {}\".format(word1, word2, word3, word4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "man : he :: woman : she\nman : he :: woman : never\nman : he :: woman : her\n"
    }
   ],
   "source": [
    "compute_and_print_analogy(embeddings, \"man\", \"he\", \"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "fly : plane :: sail : ship\nfly : plane :: sail : vessel\nfly : plane :: sail : boat\n"
    }
   ],
   "source": [
    "compute_and_print_analogy(embeddings, \"fly\", \"plane\", \"sail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "man : king :: woman : queen\nman : king :: woman : monarch\nman : king :: woman : throne\nman : king :: woman : elizabeth\n"
    }
   ],
   "source": [
    "compute_and_print_analogy(embeddings, \"man\", \"king\", \"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "man : doctor :: woman : nurse\nman : doctor :: woman : physician\nman : doctor :: woman : pregnant\n"
    }
   ],
   "source": [
    "compute_and_print_analogy(embeddings, \"man\", \"doctor\", \"woman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the Sequence Vectorizer\n",
    "\n",
    "The Sequence Vectorizer prepares the input sequence in the format expected by the Embedding Layer. The Embedding Layer is a PyTorch modelu that encapsulates the embedding matrix. The Embedding Layer enables us to map a token's indeger index (in the Vocabulary) to the vector that is further used in the neural network computation.\n",
    "\n",
    "Therefore, the input sequence should encoded as sequence of token's indices in the Vocabulary, instead of one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the preprocesed dataset\n",
    "absFilePathToPreprocessedDataset = os.path.join(fileDir, '../Data/training.1600000.processed.noemoticon_preprocessed.csv')\n",
    "pathToPreprocessedDataset = os.path.abspath(os.path.realpath(absFilePathToPreprocessedDataset))\n",
    "print (pathToPreprocessedDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TwitterDataset import TwitterDataset\n",
    "\n",
    "# Step #1: Instantiate the dataset\n",
    "# instantiate the dataset\n",
    "dataset = TwitterDataset.load_dataset_and_make_vectorizer(pathToPreprocessedDataset, representation=\"indices\")\n",
    "# get the vectorizer\n",
    "vectorizer = dataset.get_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 2,  1, 42, 79,  3], dtype=int64)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize the text of the tweet\n",
    "vectorizer.vectorize(text=\"Jerry is good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}