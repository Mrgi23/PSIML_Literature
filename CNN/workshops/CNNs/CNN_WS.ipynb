{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common ML pipeline using CNNs\n",
    "\n",
    "In this tutorial we're going to see a common ML/deep learning pipeline.\n",
    "It consists of the following steps:\n",
    "1. preprocessing and visualizing/understanding your data\n",
    "2. choosing a good model (off-the-shelf or custom?) and a loss function\n",
    "3. training and monitoring the model\n",
    "\n",
    "And it involves **a lot of iteration.**\n",
    "\n",
    "Remember: **docs, GitHub, arxiv are your friends.**\n",
    "\n",
    "---\n",
    "\n",
    "The problem we're going to \"solve\" today is to train a model to **classify** **ants** and **bees**.\n",
    "\n",
    "<img src=\"img/ant.jpg\" alt=\"an ant image\" align=\"left\" style=\"width: 350px;\"/>\n",
    "<img src=\"img/bee.jpg\" alt=\"a bee image\" align=\"center\" style=\"width: 350px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 1: Import libraries\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python's native libraries\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "# deep learning/vision libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import cv2 as cv  # OpenCV\n",
    "\n",
    "# numeric and plotting libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries are your best friends\n",
    "-----\n",
    "\n",
    "If you know how to use these you can create awesome things even if you're not an expert in mathematics, statistics, ML, deep learning, etc.\n",
    "\n",
    "### NumPy - used literally everywhere\n",
    "Either explicitly like OpenCV uses it internally or implicitly with PyTorch, TensorFlow\n",
    "<img src=\"img/numpy.png\" alt=\"numpy\" style=\"width: 350px;\"/>\n",
    "\n",
    "### OpenCV - the go-to computer vision library\n",
    "Image processing (resize, crop, etc.), even some basic ML/deep learning algorithms for imagery\n",
    "<img src=\"img/opencv.png\" alt=\"opencv\" style=\"width: 200px;\"/>\n",
    "\n",
    "### Scikit-learn - used for more classic ML algorithms \n",
    "(linear regression, PCA, Gaussian Mixture Models, SVM, etc.)\n",
    "<img src=\"img/scikit_learn.png\" alt=\"scikit learn\" style=\"width: 350px;\"/>\n",
    "\n",
    "### PyTorch - used for deep learning (neural networks)\n",
    "The most popular framework for deep learning aside from *TensorFlow*\n",
    "<img src=\"img/pytorch.jpeg\" alt=\"pytorch\" style=\"width: 350px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 2: Prepare the data\n",
    "---------\n",
    "\n",
    "We have about 120 training and 75 validation images for each class (ants & bees). Usually, this is a very small dataset to generalize upon, if trained from scratch. \n",
    "Since we are using transfer learning, we should be able to generalize reasonably well.\n",
    "\n",
    "---\n",
    "Dataset has been donwloaded for you\n",
    "\n",
    "_Note:_ this dataset is a very small subset of the famous ImageNet dataset. It can be found here `<https://download.pytorch.org/tutorial/hymenoptera_data.zip>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use data augmentation and normalization for training and only normalization for validation\n",
    "# Data augmentation helps your model to better generalize to unseen images\n",
    "\n",
    "#\n",
    "# There are broadly 2 types of augmentations for images: photometric and geometric augmentations\n",
    "#\n",
    "IMAGENET_MEAN_1 = np.array([0.485, 0.456, 0.406])  # iterate over ImageNet's images and calculate these statistics\n",
    "IMAGENET_STD_1 = np.array([0.229, 0.224, 0.225]) \n",
    "\n",
    "# These will be executed over every training/val image (handled by PyTorch's dataloader)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(IMAGENET_MEAN_1, IMAGENET_STD_1)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(IMAGENET_MEAN_1, IMAGENET_STD_1)\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'hymenoptera_data/'\n",
    "\n",
    "# basic error checking to check whether you correctly unzipped the dataset into the working directory\n",
    "assert os.path.exists(data_dir), f'Could not find {data_dir} in working directory {os.getcwd()}n'\n",
    "dirs_exist = [os.path.exists(os.path.join(data_dir, x)) for x in ['train', 'val']]\n",
    "assert all(dirs_exist), f'Could not find train/val dirs check if you have train and val directly under {data_dir}.'\n",
    "\n",
    "# ImageFolder is a PyTorch class - it expects <class1-name>, <class2-name>, ...folders under the root path you give it\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "\n",
    "# Dataloaders conviniently wraps up batch size and other details into a single object \n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True) for x in ['train', 'val']}\n",
    "\n",
    "#\n",
    "# todo: (#1) get a single batch of images from the train dataloader and print it's shape\n",
    "#\n",
    "\n",
    "# cuda:0 is the 1st GPU on your system if you had more than 1 you could use the 2nd by setting cuda:1, etc.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}, (cuda:0 is prefered over CPU).\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 3: Understand your data (visualizations)\n",
    "---------\n",
    "\n",
    "Important terms:\n",
    "**Ground Truth (GT)** - correct labels\n",
    "\n",
    "Created either:\n",
    "1. Purely by humans - somebody sits down, given instructions, follows those instructions and labels the data (boring)\n",
    "2. Semi-automatic - a part of the labels are produced by some algorithms, humans just slightly modify those\n",
    "3. Fully-automatic - some kind of autonomous pipeline that creates the GT labels\n",
    "\n",
    "_Note: if we have synthetic data (SX) than we're always getting GT \"for free\" i.e. the 3rd option above._\n",
    "\n",
    "Extremely laborous semantic segmentation labeling:\n",
    "\n",
    "<img src=\"img/gt.png\" alt=\"ground truth for semantic segmentation\" align=\"center\" style=\"width: 450px;\"/>\n",
    "\n",
    "\n",
    "In our case somebody had to go through images and click either bee or ant - it's error prone so usually every image is passed to multiple annotators and usually by majority-voting we get the correct label (with high probability).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_tensor(tensor, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    # convert PyTorch Tensor to numpy and change from channel first to channel last (2 common image layouts HWC & CHW)\n",
    "    # similar to RBG<->BGR confusion...\n",
    "    img = tensor.numpy().transpose((1, 2, 0))\n",
    "    img = IMAGENET_STD_1 * img + IMAGENET_MEAN_1 # denormalize\n",
    "    #\n",
    "    # todo: (#4) find min and max element and try and see what the range is before and after clip\n",
    "    #\n",
    "    img = np.clip(img, 0, 1)  \n",
    "    \n",
    "    plt.imshow(img)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "print(f'Training dataset size: {dataset_sizes[\"train\"]} images, Validation dataset size: {dataset_sizes[\"val\"]} images')\n",
    "\n",
    "#\n",
    "# todo: (#2) (understanding how ImageFolder works) add dummy class under train/ and run again\n",
    "#\n",
    "class_names = image_datasets['train'].classes\n",
    "print(f\"Classes in our training/val datasets: {class_names}\")\n",
    "\n",
    "img_batch, classes = next(iter(dataloaders['train'])) # Get a batch of training data\n",
    "print(f'Shape of batch of images: {img_batch.shape}')\n",
    "\n",
    "image_grid = torchvision.utils.make_grid(img_batch) # Make an image grid from batch\n",
    "print(f'Image grid shape={image_grid.shape}')  # Knowing shapes of your tensors is helpful for debugging!\n",
    "\n",
    "#\n",
    "# todo: (#3) add some photometric transform to train dataloader and run again (brightness, contrast, saturation, hue)\n",
    "# What do you notice happened? Why would we want to do this?\n",
    "#\n",
    "\n",
    "# Ground truth is the correct label of the image - contrast that with predicted label that the model outputs\n",
    "imshow_tensor(image_grid, title='Ground truth: ' + str([class_names[x] for x in classes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 4: Define the core util functions\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=25):\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    metrics = defaultdict(list)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    \n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data).item()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = float(running_corrects) / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            metrics[phase+\"_loss\"].append(epoch_loss)\n",
    "            metrics[phase+\"_acc\"].append(epoch_acc)\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - start_time\n",
    "    print(f'Training complete in {(time_elapsed // 60):.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print('Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize model predictions\n",
    "---------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            tmp, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow_tensor(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)\n",
    "        \n",
    "# Count how many trainable weights the model has <- just for having a feeling for how big the model is\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def display_metrics_charts(metrics):\n",
    "    plt.subplots_adjust(wspace=1, hspace=1)\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2)\n",
    "    keys = list(metrics.keys())\n",
    "    cnt = 0\n",
    "    for row in ax:\n",
    "        for col in row:\n",
    "            col.set_title(keys[cnt])\n",
    "            col.plot(metrics[keys[cnt]])\n",
    "            cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Off-the-shelf vs custom models\n",
    "----------------------\n",
    "\n",
    "There are many popular models in torchvision - like **ResNets**, **VGG**, **MobileNets**, etc.\n",
    "\n",
    "But sometimes you may wish to create your own model - because it's SOTA and hasn't yet been integrated into PyTorch.\n",
    "\n",
    "![title](img/resnetvgg.png)\n",
    "\n",
    "step 5: Fine-tuning off-the-shelf models (transfer learning)\n",
    "----------------------------------\n",
    "\n",
    "Here, we need to freeze all of the layers of the network except the final layer. \n",
    "\n",
    "We need to set ``requires_grad == False`` to freeze the parameters so that the\n",
    "gradients are not computed in ``backward()``.\n",
    "\n",
    "Loss functions are really important! Our model is as good as our loss function.\n",
    "If it correlates perfectly with the performance we want our model to have, super! If not well we'll have to have some kind of visual inspections as the curves themselves won't cut it.\n",
    "\n",
    "We'll be using this loss function (**cross entropy**) for classification:\n",
    "![cross entropy loss](img/cross_entropy.png)\n",
    "\n",
    "**Arxiv and GitHub are your good friends!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# First just run this cell\n",
    "#\n",
    "\n",
    "SUPPORTED_MODELS = ['resnet18', 'vgg16', 'mnetv2']\n",
    "model_name = SUPPORTED_MODELS[0]\n",
    "\n",
    "if model_name == SUPPORTED_MODELS[0]:\n",
    "    model = torchvision.models.resnet18(pretrained=True)\n",
    "elif model_name == SUPPORTED_MODELS[1]:\n",
    "    model = torchvision.models.vgg16(pretrained=True)\n",
    "elif model_name == SUPPORTED_MODELS[2]:\n",
    "    model = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "else:\n",
    "    raise Exception(f'Model {model_name} not supported yet.')\n",
    "\n",
    "print(f\"Using {model_name}, number of trainable params before freezing {count_parameters(model)}\")\n",
    "    \n",
    "# Freeze parameters - making them non-trainable\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "finetuned_model = copy.deepcopy(model)\n",
    "if model_name == SUPPORTED_MODELS[0]:\n",
    "    #\n",
    "    # todo: (#5) open https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py to understand why fc\n",
    "    #\n",
    "    num_ftrs = finetuned_model.fc.in_features\n",
    "    finetuned_model.fc = nn.Linear(num_ftrs, 2)\n",
    "elif model_name == SUPPORTED_MODELS[1]:\n",
    "    # todo: (#6) modify model so as to have fully-connected layer at it's output for binary classification\n",
    "    print('to implement.')\n",
    "elif model_name == SUPPORTED_MODELS[2]:\n",
    "    # todo: (#7) modify model so as to have fully-connected layer at it's output for binary classification\n",
    "    print('to implement.')\n",
    "\n",
    "print(f\"Num of params after freezing: {count_parameters(finetuned_model)}\")\n",
    "\n",
    "model = model.to(device)\n",
    "finetuned_model = finetuned_model.to(device)  # place the model onto the GPU (you hopefully have one)\n",
    "\n",
    "#\n",
    "# todo: (#8) whats the biggest model of the 3?\n",
    "#\n",
    "\n",
    "# loss function - this pretty much defines the model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#\n",
    "# todo: (#9) open https://arxiv.org/pdf/1512.03385.pdf <- originally used SGD we'll stick with Adam, why?\n",
    "#\n",
    "# We optimize only the trainable params\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, finetuned_model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict after fine-tuning procedure\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Run the cell to start training your model\n",
    "#\n",
    "num_epochs = 2\n",
    "\n",
    "# change num_epochs to smaller number for faster iteration\n",
    "finetuned_model, metrics = train_model(finetuned_model, criterion, optimizer, num_epochs)  \n",
    "display_metrics_charts(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Run this cell to qualitatively assess model's performance \n",
    "#\n",
    "visualize_model(finetuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 6: Using a custom (CNN) model\n",
    "-----\n",
    "\n",
    "Generic off-the-shelf models are good. But in industry when you want to create the best product for your users,\n",
    "you want to use custom models which were designed for that very specific task! (and not generic image classification models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Custom models inherit from nn.Module and you define 2 functions in their interface __init__ and forward\n",
    "#\n",
    "\n",
    "# Note: this is also a toy example but it showcases the workflow of developing a new architecture\n",
    "class CustomCNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        num_of_channels = [3, 16, 32, 64, 128]\n",
    "        kernel_sizes = [3, 3, 3, 3]\n",
    "        stride_sizes = [2, 2, 2, 2]\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(num_of_channels[0], num_of_channels[1], kernel_size=kernel_sizes[0], stride=stride_sizes[0])\n",
    "        self.conv2 = nn.Conv2d(num_of_channels[1], num_of_channels[2], kernel_size=kernel_sizes[1], stride=stride_sizes[1])\n",
    "        self.conv3 = nn.Conv2d(num_of_channels[2], num_of_channels[3], kernel_size=kernel_sizes[2], stride=stride_sizes[2])\n",
    "        self.conv4 = nn.Conv2d(num_of_channels[3], num_of_channels[4], kernel_size=kernel_sizes[3], stride=stride_sizes[3])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Linear(num_of_channels[-1] * 7 * 7, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.relu(self.conv1(x))\n",
    "        y = self.relu(self.conv2(y))\n",
    "        y = self.relu(self.conv3(y))\n",
    "        y = self.relu(self.conv4(y))\n",
    "        y = self.avgpool(y)\n",
    "        y = torch.flatten(y, 1)\n",
    "        y = self.classifier(y)\n",
    "        return y\n",
    "    \n",
    "custom_cnn = CustomCNN().to(device)\n",
    "optimizer_conv = optim.Adam(filter(lambda p: p.requires_grad, custom_cnn.parameters()))\n",
    "\n",
    "print(f\"number of params in model {count_parameters(custom_cnn)}\")\n",
    "\n",
    "#\n",
    "# todo: (#10) Why is acc 0.5? Anybody?\n",
    "#\n",
    "model_conv, metrics = train_model(custom_cnn, criterion, optimizer_conv, num_epochs=25)\n",
    "\n",
    "plt.subplots_adjust(wspace=1, hspace=1)\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2)\n",
    "keys = list(metrics.keys())\n",
    "cnt = 0\n",
    "for row in ax:\n",
    "    for col in row:\n",
    "        col.set_title(keys[cnt])\n",
    "        col.plot(metrics[keys[cnt]])\n",
    "        cnt += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 7: Demystyfing CNNs - visualizations\n",
    "-----\n",
    "We are going to hook up our function to all layers, so that we save their outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model_name == SUPPORTED_MODELS[1], 'Visualization should be done on VGG.' \n",
    "assert num_epochs >= 25, 'Visualization should be done on network trained at least with 25 epochs.'\n",
    "\n",
    "# This will allow us to visualize what each layer learned as well as what it outputed (activations)\n",
    "visualization = {}\n",
    "\n",
    "def hook_fn(m, inp, outp):\n",
    "  visualization[m] = outp\n",
    "\n",
    "def register_visualization_hooks(net):\n",
    "    for name, layer in net._modules.items():\n",
    "        # If it is a sequential, don't register a hook on it\n",
    "        # but recursively register hook on all it's module children\n",
    "        if isinstance(layer, nn.Sequential):\n",
    "            register_visualization_hooks(layer)\n",
    "        else:\n",
    "            # it's a non sequential. Register a hook\n",
    "            layer.register_forward_hook(hook_fn)\n",
    "\n",
    "register_visualization_hooks(finetuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to visualize few convolutional layer kernels (filters) from the finetuned vgg16 model from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def visualize_kernels(visualization):\n",
    "    display_number_of_layers = 3\n",
    "    for i, layer in enumerate(visualization):\n",
    "        \n",
    "        # we will skip all non-conv layers\n",
    "        if not isinstance(layer, nn.Conv2d):\n",
    "            continue\n",
    "        \n",
    "        if display_number_of_layers == 0:\n",
    "            return\n",
    "                \n",
    "        # Normalize filter values to 0-1 so we can visualize them:\n",
    "        weights = layer.weight\n",
    "        f_min, f_max = weights.min(), weights.max()\n",
    "        weights = (weights - f_min) / (f_max - f_min)\n",
    "\n",
    "        # Let's transpose array so that we can visualize it easier\n",
    "        weights = weights.detach().cpu().numpy().transpose((3, 2, 1, 0))\n",
    "        \n",
    "        print(f'Layer {i} with structure {layer}')\n",
    "        n_filters, ix = 6, 1\n",
    "        for i in range(n_filters):\n",
    "            # get the filter\n",
    "            f = weights[:, :, :, i]\n",
    "            # plot each channel separately\n",
    "            for j in range(3):\n",
    "                # specify subplot and turn of axis\n",
    "                ax = plt.subplot(n_filters, 3, ix)\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                # plot filter channel in grayscale\n",
    "                plt.imshow(f[:, :, j], cmap='gray')\n",
    "                ix += 1\n",
    "        plt.show()\n",
    "        display_number_of_layers -= 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dark squares will indicate small or inhibitory weights, and the light squares represent large or excitatory weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualization_image = next(iter(dataloaders['val']))[0][0].unsqueeze(0)\n",
    "# run the image through the model to gather kernel/filter visualizations (we're expecting finetuned_model to be vgg16 here)\n",
    "finetuned_model(visualization_image.to(device))\n",
    "\n",
    "visualize_kernels(visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of kernels can be informative on first, maybe second conv layer, however later on it does not tell us much, and it can be hard to interpret. Another informative thing we can do in order to understand CNNs better, is to visualize the activations given a specific image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_maps(visualization):\n",
    "    for i, layer in enumerate(visualization):\n",
    "        # we will visualize only conv layers\n",
    "        if not isinstance(layer, nn.Conv2d):\n",
    "            continue\n",
    "        \n",
    "        activations = visualization[layer].cpu()\n",
    "        plt.matshow(activations[0,0, :, :], cmap='viridis')\n",
    "        plt.title(f'Layer {i} Format: {layer} \\n')\n",
    "        if i == 0:\n",
    "            for j in range(1, 4):\n",
    "                plt.matshow(activations[0,j, :, :], cmap='viridis')\n",
    "                plt.title(f'Layer {i} Format: {layer} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's feed an image to the model in order to see filter activations for it. We will now show that image, as well as display filter activations on the given image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow_tensor(visualization_image.cpu()[0], '___________________Original Image inputed into the first laye___________________')\n",
    "\n",
    "visualize_feature_maps(visualization)\n",
    "imshow_tensor(visualization_image.cpu()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, in the first layer fitlers are catching pretty simple patterns (e.g. horizontal edges), however, going into the deeper layers, the patterns get more complicated and no longer resamble the input image in any way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also one more visualization way of understanding CNNs, which is by visualizing the image that would activate network the most for a given class or layer. This is also achieved by minimizing the loss, however not by modifying the weights of the layers but the image itself!\n",
    "\n",
    "Below you can find examples which were made by following the strategy explained above. Authors have started from certain input image, and modified it in a way to maximise the activations of certain layers/classes and got following results (Examples from Deep Dream):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/deepdreamdoggo.png\" alt=\"deep dream doggo\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/deepdreambird.png\" alt=\"deep dream bird\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/deepdreamlisa.png\" alt=\"deep dream Mona Lisa\" style=\"width: 600px;\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
